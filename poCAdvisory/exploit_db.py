from bs4 import BeautifulSoup
import json
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import requests
import re
import os
import datetime
import sys
import configparser
import time
from lib.dateConvert import dateConvert          
import json
import psycopg2                                                                                                                                 


class moniExploitDB():
    def __init__(self):
        self.daily = True
        self.settings = configparser.ConfigParser()
        self.settings.read('config.ini')
        hostName = self.settings.get('database', 'host')
        userName = self.settings.get('database', 'user')
        password = self.settings.get('database', 'pass')
        databaseName = self.settings.get('database', 'dbname')

        self.connection = psycopg2.connect(user=userName,password=password,host=hostName,port="5432",database=databaseName)
        self.cursor = self.connection.cursor()   

    def maxExploitID(self):
        cmd = 'select MAX(app_id) from PoCReference_DB where application="exploit-db"'
        cur = self.mydb.cursor()
        cur.execute(cmd)
        maxID = cur.fetchall()[0][0]
        return maxID

    def getPageSource(self, url):
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument('ignore-certificate-errors')
        driver = webdriver.Chrome('chromedriver', chrome_options=chrome_options)

        driver.set_page_load_timeout(120)
        driver.set_script_timeout(120)
        driver.get(url)
        html_source = driver.page_source
        driver.close()
        return html_source


    def initialize(self, date_update):
        date_update = date_update

        url = "https://www.exploit-db.com/rss.xml"
        page = requests.get(url)
        soup = BeautifulSoup(page.content, "html.parser")

        if re.findall(r'https:\/\/www\.exploit-db\.com\/exploits\/(\d+)', str(soup)):
            expIds = re.findall(r'https:\/\/www\.exploit-db\.com\/exploits\/(\d+)', str(soup))

        if len(expIds) > 0:
            for expId in expIds:
                url = "https://www.exploit-db.com/exploits/%s" % expId
                headers = requests.utils.default_headers()
                headers.update({
                    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0',
                })

                while True:
                    try:
                        page = requests.get(url, headers=headers)
                        soup = BeautifulSoup(page.content, "html.parser")
                        break
                    except:
                        time.sleep(60)
                        pass

                if not re.findall(r'404 Page Not Found', str(soup)):
                    p = 0
                    title = soup.findAll('title')
                    h3 = soup.findAll('h4',{'class':'info-title'})
                    h6 = soup.findAll('h6',{'class':'stats-title'})

                    h3Array = []
                    h6Array = []
                    for h in h3:
                        h3Array.append((h.text).strip())
                    for h in h6:
                        h6Text = h.text.strip().replace('\n\n', '').replace('\n', '')
                        h6Array.append(re.sub(' +', ',', str(h6Text)))

                    dictA = dict(zip(h3Array, h6Array))

                    if len(dictA) != 0:
                        name = title[0].text
                        vuln_name = name
                        vuln_name = vuln_name.replace("'", "")
                        description = name
                        description = description.replace("'", "")
                        pub_date = dictA['Date:']
                        try:
                            res1 = dateConvert()
                            pub_date = res1.dateCon(pub_date)
                        except:
                            pass
                        cve_id = "%s" % dictA['CVE:']
                        cve_ids = cve_id.split(',')
                        type = dictA['Type:']
                        platform = dictA['Platform:']
                        exp_id = dictA['EDB-ID:']
                        author_name = dictA['Author:']
                        reference = "http://www.exploit-db.com/download/%s" % (dictA['EDB-ID:'])
                        cve_re = re.compile(r"CVE\W\w{4}\W\w+")

                        details = {}
                        details['type'] = type
                        details['platform'] = platform

                        for cve_id in cve_ids:
                            cve_id = "CVE-%s" % cve_id
                            cmd = """INSERT INTO PoCReference_DB("application", "app_id", "cve_id", "description", "vuln_name", "details", "publish_date", "reference", "last_update")VALUES ('{application}', '{exp_id}', '{cve_id}', '{description}', '{vuln_name}', '{details}', '{pub_date}', '{reference}', '{date_update}')ON CONFLICT("cve_id", "app_id", "application")DO UPDATE SET ("description", "vuln_name", "details", "publish_date", "reference", "last_update") = ('{description}', '{vuln_name}', '{details}', '{pub_date}', '{reference}', '{date_update}');""".format(exp_id=exp_id, cve_id=cve_id, description=description, details=json.dumps(details), vuln_name=vuln_name, pub_date=pub_date, reference=reference, date_update=date_update, application='exploit-db')
                            print(cmd)
                            self.cursor.execute(cmd)
                            self.connection.commit()
    
if __name__ == "__main__":
    now = datetime.datetime.now()
    date_update = "%s" % now.strftime("%Y-%m-%d %H:%M:%S")
    res = moniExploitDB()
    res.initialize(date_update)
